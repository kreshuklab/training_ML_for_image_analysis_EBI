{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "master_script.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vzinche/training_ML_for_image_analysis_EBI/blob/master/master_script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD-2SXCuL_fx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from scipy.ndimage import binary_erosion\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms, utils\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCOLYPt1MT83",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "class NucleiDataset(Dataset):\n",
        "    \"\"\" Nuclei and masks \"\"\"\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.samples = os.listdir(root_dir)\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "        self.inp_transforms = transforms.Compose([transforms.Grayscale(),\n",
        "                                                  transforms.ToTensor(),\n",
        "                                                  transforms.Normalize([0.5], [0.5])\n",
        "                                                  ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.samples[idx],\n",
        "                                'images', self.samples[idx]+'.png')\n",
        "        image = Image.open(img_name)\n",
        "        image = image.convert(\"RGB\")\n",
        "        image = self.inp_transforms(image)\n",
        "        masks_dir = os.path.join(self.root_dir, self.samples[idx], 'masks')\n",
        "        if not os.path.isdir(masks_dir):\n",
        "            return image\n",
        "        masks_list = os.listdir(masks_dir)\n",
        "        mask = torch.zeros(1, len(image[0]), len(image[0][0]))\n",
        "        for mask_name in masks_list:\n",
        "            one_nuclei_mask = Image.open(os.path.join(masks_dir, mask_name))\n",
        "            one_nuclei_mask = binary_erosion(one_nuclei_mask).astype('float32')\n",
        "            one_nuclei_mask = self.to_tensor(one_nuclei_mask[..., np.newaxis])\n",
        "            mask += one_nuclei_mask\n",
        "        if self.transform is not None:\n",
        "            image, mask = self.transform([image, mask])\n",
        "        return image, mask\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPrn-YxeMXzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def focal_loss(prediction, mask, gamma=2):\n",
        "    loss = F.binary_cross_entropy_with_logits(prediction, mask, reduce=False)\n",
        "    invprobs = F.logsigmoid(-prediction * (mask * 2 - 1))\n",
        "    loss = (invprobs * gamma).exp() * loss\n",
        "    return loss.mean()\n",
        "\n",
        "\n",
        "def iterate(data):\n",
        "    return tqdm(data, desc='Samples processed', bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt}')\n",
        "\n",
        "\n",
        "def get_iou(prediction, mask):\n",
        "    iou = (prediction & mask).sum().float() / (prediction | mask).sum().float()\n",
        "    return iou\n",
        "\n",
        "\n",
        "def show_images(inp, mask, pred):\n",
        "    f, axarr = plt.subplots(1, 3)\n",
        "    axarr[0].imshow(inp[0][0])\n",
        "    axarr[1].imshow(mask[0][0])\n",
        "    axarr[2].imshow(pred[0][0])\n",
        "    _ = [ax.axis('off') for ax in axarr]\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def show_dataset(dataset):\n",
        "    idx = np.random.randint(0, len(dataset))\n",
        "    img, mask = dataset[idx]\n",
        "    f, axarr = plt.subplots(1, 2)\n",
        "    axarr[0].imshow(img[0])\n",
        "    axarr[1].imshow(mask[0])\n",
        "    _ = [ax.axis('off') for ax in axarr]\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "class RandomCrop(object):\n",
        "    \"\"\"Crop randomly the input image and the output mask\"\"\"\n",
        "    def __init__(self, crop_size):\n",
        "        assert isinstance(crop_size, (int, tuple, list))\n",
        "        if isinstance(crop_size, int):\n",
        "            self.output_size = (crop_size, crop_size)\n",
        "        else:\n",
        "            assert len(crop_size) == 2\n",
        "            self.crop_size = crop_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        assert len(sample) == 2\n",
        "        image, mask = sample\n",
        "        w, h = image.shape[1:]\n",
        "        new_w, new_h = self.output_size\n",
        "        top = np.random.randint(0, h - new_h) if h - new_h > 0 else 0\n",
        "        left = np.random.randint(0, w - new_w) if w - new_w > 0 else 0\n",
        "        image = image[:, left: left + new_w, top: top + new_h]\n",
        "        mask = mask[:, left: left + new_w, top: top + new_h]\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "def evaluate(dataloader, model, device='cpu'):\n",
        "    print ('Evaluating!')\n",
        "    dataset_size = len(dataloader)\n",
        "    test_accuracy = 0.0\n",
        "    test_iou = 0.0\n",
        "    count = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, masks in iterate(dataloader):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "            count += 1\n",
        "            outputs = model(images)\n",
        "            predictions = (outputs > 0.5)\n",
        "            accuracy = torch.mean((predictions == masks.byte()).float())\n",
        "            iou = get_iou(predictions, masks.type(torch.bool))\n",
        "            test_accuracy += accuracy.item()\n",
        "            test_iou += iou.item()\n",
        "    epoch_accuracy = test_accuracy / dataset_size\n",
        "    epoch_iou = test_iou / dataset_size\n",
        "    print('Evaluation iou is {:.6f}, accuracy is {:.6f}'.format(epoch_iou, epoch_accuracy))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8ubfbQTMbmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UNetBlock(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(UNetBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_dim, out_dim, kernel_size=3, padding=1)\n",
        "        self.norm1 = nn.BatchNorm2d(out_dim)\n",
        "        self.conv2 = nn.Conv2d(out_dim, out_dim, kernel_size=3, padding=1)\n",
        "        self.norm2 = nn.BatchNorm2d(out_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.norm1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.norm2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNetBlockDown(UNetBlock):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(UNetBlockDown, self).__init__(in_dim, out_dim)\n",
        "        self.pool = nn.MaxPool2d(2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(x)\n",
        "        x = super().forward(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNetBlockUp(UNetBlock):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(UNetBlockUp, self).__init__(in_dim, out_dim)\n",
        "        self.up_conv = nn.ConvTranspose2d(in_dim, out_dim, stride=2, kernel_size=2)\n",
        "        self.up_norm = nn.BatchNorm2d(out_dim)\n",
        "\n",
        "    def forward(self, x, saved_x):\n",
        "        x = self.up_conv(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.up_norm(x)\n",
        "        x = torch.cat((x, saved_x), 1)\n",
        "        x = super().forward(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_filters, num_layers):\n",
        "        super(UNet, self).__init__()\n",
        "        self.first_layer = UNetBlock(1, in_filters)\n",
        "        self.blocks_down = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.blocks_down.append(UNetBlockDown(in_filters, in_filters*2))\n",
        "            in_filters *= 2\n",
        "        self.blocks_up = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.blocks_up.append(UNetBlockUp(in_filters, in_filters//2))\n",
        "            in_filters //= 2\n",
        "        self.last_layer = nn.Sequential(\n",
        "            nn.Conv2d(in_filters, 1, kernel_size=1, padding=0), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        saved_x = []\n",
        "        x = self.first_layer(x)\n",
        "        for block in self.blocks_down:\n",
        "            saved_x.append(x)\n",
        "            x = block(x)\n",
        "        for i in range(len(self.blocks_up)):\n",
        "            x = self.blocks_up[i](x, saved_x[-1-i])\n",
        "        x = self.last_layer(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q089p1qyMeb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1tyI7ig2obOxAdEnKBrUXFD7uZQX9tRKD' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1tyI7ig2obOxAdEnKBrUXFD7uZQX9tRKD\" -O nuclei_data.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip -qq nuclei_data.zip && rm nuclei_data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_8GkEjgM45l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1O66UElt2ZfhLXUKKX_nTxmIXh6fMA2rT' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1O66UElt2ZfhLXUKKX_nTxmIXh6fMA2rT\" -O kaggle_data.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip -qq kaggle_data.zip && rm kaggle_data.zip\n",
        "!mkdir nuclei_train_data && unzip -qq stage1_train.zip -d nuclei_train_data/ && rm stage1_train.zip\n",
        "!mkdir nuclei_test_data && n=0 && for file in nuclei_train_data/*; do test $n -eq 0 && mv \"$file\" nuclei_test_data/; n=$((n+1)); n=$((n%4)); done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxigbN48NwgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls nuclei_train_data | wc -l && ls nuclei_test_data | wc -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6pFV8wcO2im",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -ltrh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryWEN8LgM4yH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_DATA_PATH = 'nuclei_train_data'\n",
        "train_data = NucleiDataset(TRAIN_DATA_PATH, RandomCrop(256))\n",
        "train_dataloader = DataLoader(train_data, batch_size=5, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wwul0qJXNyRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_dataset(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIozWvPXj2_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST_DATA_PATH = 'nuclei_test_data'\n",
        "test_data = NucleiDataset(TEST_DATA_PATH, RandomCrop(256))\n",
        "test_dataloader = DataLoader(test_data, batch_size=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1JVHmjNj7r5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_dataset(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flWvk4BNkAP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_LAYERS = 3    # determines the capacity and the 'depth' (filed of view) of the network\n",
        "IN_FILTERS = 32   # the number of the feature maps in the first layer - also affects the capacity\n",
        "model=UNet(IN_FILTERS, NUM_LAYERS)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hnWETz3kF1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, train_loader, test_loader, num_epochs=10, device='cpu', writer=None):\n",
        "    dataset_size = len(train_loader)  #how many samples we have\n",
        "    model = model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "        model.train()\n",
        "        train_loss = 0.0    # the value that we will backprop\n",
        "        train_accuracy = 0.0    # just a helpful (for us) metric\n",
        "        train_iou = 0.0    # another helpful metric\n",
        "        count = 0\n",
        "        for images, masks in iterate(train_loader):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "            count += 1\n",
        "            optimizer.zero_grad()    # erase all the gradient from the previous steps\n",
        "            outputs = model(images)    # predict\n",
        "            predictions = (outputs > 0.5)    # binarize the predictions to get a mask\n",
        "            if count % 10 == 0:    # every tenth iteration show how we are performing\n",
        "                show_images(images.cpu(), masks.cpu(), predictions.cpu())\n",
        "            loss = focal_loss(outputs, masks)    # calculate the loss between the predictions and the ground truth\n",
        "            accuracy = torch.mean((predictions == masks).float())    # how much is the prediction similar to the ground truth? pixelwise\n",
        "            iou = get_iou(predictions, masks.type(torch.bool))    # calculate the intersection over union (explained below)\n",
        "            loss.backward()    # compute the gradients for every neuron\n",
        "            optimizer.step()    # update the weights!\n",
        "            if writer:\n",
        "              writer.add_scalar('training loss', loss.item(), epoch * dataset_size + count)\n",
        "              writer.add_scalar('training accuracy', accuracy.item(), epoch * dataset_size + count)\n",
        "              writer.add_scalar('training iou', iou.item(), epoch * dataset_size + count)\n",
        "            train_loss += loss.item()\n",
        "            train_accuracy += accuracy.item()\n",
        "            train_iou += iou.item()\n",
        "        epoch_loss = train_loss / dataset_size\n",
        "        epoch_accuracy = train_accuracy / dataset_size\n",
        "        epoch_iou = train_iou / dataset_size\n",
        "        print('Training loss is {:.6f}, iou is {:.6f}, accuracy is {:.6f}'.format(epoch_loss, epoch_iou, epoch_accuracy))\n",
        "        evaluate(test_loader, model, device)    # every epoch we want to check how the model performs on a previously unseen data\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb6NevRXoxX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "writer = SummaryWriter()\n",
        "#writer.add_graph(model, test_dataloader.__iter__().__next__()[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b28QHCLLrBTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1AGBjXxkILn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = train(model, optimizer, train_dataloader, test_dataloader, num_epochs=10, device=device, writer=writer)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}